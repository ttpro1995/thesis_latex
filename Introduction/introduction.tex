\hypertarget{chap:intro}{\chapter{Introduction}}
\section{Overview}
\subsection{Sentiment Analysis and its applications}
\subsubsection{Definition}
In the nutshell, sentiment analysis is to determine whether the opinion about a specific product, event, organization is positive or negative.

\subsubsection{Applications}
Before a customer buys any product, the decision of whether or not he will buy it depends largely on his prior opinions about that product. 
These opinions, in turn, have been built based on his opinions on relating companies or products and other customers' opinions about that product.  
After having been experiencing the product, his posterior opinions on it not only tell us about which features he like or dissatisfy with.
They also inform us about the reasons why he bought it, his expectations, needs and even more detailed information about him.
In a circle, his opinions will also affect the opinion of new customers and even the design of future products.
As a result, customers' opinions are the controllers behind every companies' good decisions. 
They shape companies' marketing strategies, policies, and designs of products.
They judge which company is more competent than another.

A long time ago, when companies needed to know opinions of their customers, they conducted surveys, opinion polls and focus groups~\cite{liu2012sentiment}. 
In recent years, thanks to the dramatic growth of social media, customers' opinions are expressed in the highest speed and volume ever recorded in history.
With this amount of data, it is inefficient to read and analyze or even collect them manually.

\subsubsection{Different levels in sentiment analysis}
\paragraph{Document-level}
Document-level sentiment analysis is to determine whether a document (usually a full review) about a specific entity (a product, location, service, ...) is positive or negative. For example,~\cite{pang2002thumbs} perform document-level sentiment analysis on movies review data.
\paragraph{Sentence-level}\label{sec:sent-level}
Sentence-level sentiment analysis is to determine whether a sentence expressed positive or negative. 
In this thesis, we focus our study on sentence-level sentiment analysis~\cite{liu2012sentiment}.

This thesis mainly concerns with the problem of sentence-level sentiment analysis. 
Given a sentence, the task is to classify which sentiment class is expressed by it~\cite{liu2012sentiment}.
The number of classes of sentiment can vary depending on data sets or settings~\cite{Rotten-Tomato}~\cite{socher2013recursive}.

There are only two popular datasets for this task: Rotten Tomatoes Movie Review~\cite{Rotten-Tomato} and Stanford Sentiment Treebank~\cite{socher2013recursive} which was build based on the former.
For more details, the validation and the test set of the two datasets are similar, the differences only appear in training set.
Whereas Stanford Sentiment Treebank provides phrase-level labels, there are only sentence-level labels in Rotten Tomatoes Movie Review~\cite{socher2013recursive}.
Since the publication of Stanford Sentiment Treebank dataset in 2013, the dataset has been widely used in most researches as a replacement for Rotten Tomatoes Movie Review dataset~\cite{treeLSTM}~\cite{KimCNN}~\cite{cnn-rnn}~\cite{2-layer-cnn}~\cite{socher2013recursive}.
For the purpose of comparison, all of our models in this thesis were evaluated using Stanford Sentiment Treebank.

\paragraph{Aspect-level}
Aspect-level  sentiment analysis's purpose is to determine opinion, whether positive or negative, against specific aspect of entity~\cite{liu2012sentiment}.




\subsection{Deep Learning in Sentiment Analysis}
In recent years, the advancements of deep learning have led to dramatic improvements in the field of Sentiment Analysis:
\begin{description}
\item [2013] Based on the theory that learning appropriate intermediate representations can lead to better generalisation~\cite{knowledge-matter}~\cite{tran-auto-encoder}, Socher and the co-authors augmented Rotten Tomatoes Movie Review  (RT-MR) dataset~\cite{Rotten-Tomato} with phrase-level sentiment labels (the new dataset was named \hyperref[sec:sst]{Stanford Sentiment Treebank} (SST)), in this way, any network can learn to correctly classify phrase-level sentiment, before it learns to classify sentence-level sentiment~\cite{socher2013recursive}. Along with the dataset, the authors also introduced three Recursive Neural Network, which inspired by the recursive structure of language ~\cite{socher2013recursive}.
They were able to archive state of the art performance on SST test set (which is similar to RT-MR test set). 
Since then, this dataset has become the most popular dataset for the task of sentence-level sentiment analysis.

In the same year, two other studies which have a large impact on the whole NLP community appeared. 
Mikolov and his partners introduced word2vec which has the ability to learn a certain level of syntactic and semantic relationships among words~\cite{word2vec}. 
Pre-learned word presentations have been used widely and become the “secret sauce” for the success of recent NLP systems~\cite{Luong_betterword}.

Another research~\cite{GravesLSTM} popularized \hyperref[sec:lstm]{Long Short Term Memory Network (LSTM)} and its "staked" variations (e.g. \hyperref[sec:multilayer-lstm]{multilayer LSTM} , \hyperref[sec:bilstm]{Bidirectional LSTM}).
By mitigating the problem of \hyperref[sec:gradient-vanish]{gradient vanishing} of \hyperref[sec:RNN]{RNNs}, LSTMs and its variations become so popular that, we can find them in most NLP publications from 2014 to 2017. 
LSTMs is one of the first successful variation of RNNs which leads the way to many other more advanced RNNs variation~\cite{olah2016attention}.

\item [2014] Applying only \hyperref[kim-cnn]{one layer CNN on multi-channel word embeddings}, Yoon Kim successfully archived state of the art performance on \hyperref[sec:sst]{SST (binary setting)} as well as other datasets of different tasks~\cite{KimCNN}. 
This success attracted many future researches applying CNN on sentiment analysis~\cite{2-layer-cnn}~\cite{cnn-rnn}.

Another research which we applied in our thesis is Glove method for training word embeddings~\cite{glove}.
Different from word2vec, Glove vectors captures word co-occurrences globally in the corpus, whereas word2vec only captures local word co-occurrences in each window of its training examples~\cite{glove}.

\item [2015] Combining recursive structure of Recursive Neural Networks~\cite{socher2013recursive} and LSTM~\cite{originLSTM}, Kai Sheng Tai, Socher and Christopher Manning introduced \hyperref[sec:treelstm]{tree-structured LSTM}  (TreeLSTM)~\cite{treeLSTM}.
Their models archived state of the art performance on \hyperref[sec:sst]{SST (fine-grained setting)} and task of semantic relatedness (SemEval~2014, Task~1~\cite{SemeEvalTask1}).
This success lead to multiple researches~\cite{need-tree}~\cite{bowman-treevslstm}~\cite{Graves_Nature2016} which aim to \hyperref[treelstm-advantage]{comparing tree-structured and sequential LSTM}. 
The question is whether tree-structured networks are necessary or at least have some advantages over sequential architects when processing recursive-structured languages~\cite{need-tree}~\cite{bowman-treevslstm}.   

In the same year, there are two researches~\cite{ParagraphVec}~\cite{semisup-seq2seq} of Quoc V. Le which introduced several methods for \hyperref[sec:unsupervised-pretrain]{unsupervised pre-training neural network models for NLP tasks}.
These methods help network models to mitigate over-fitting (which lead to better generalization) by allowing them to be pre-trained on large unlabeled datasets.
We will apply several unsupervised pre-train methods in this thesis.

\item [2016] Xingyou Wang and his partners combined convolutional and recurrent neural networks~\hyperref[cnn-rnn]{(CNN-RNN)} to archive \hyperref[table:cnn-rnn]{staggering improvement} on SST (both binary and fine-grained setting).
Until now\footnote{July, 2017}, their models are state of the art on SST and also RT-MR~\cite{cnn-rnn}.
\end{description}  

\section{Approachs}
\subsection{Utilizing local syntactic information at each node of Recursive Neural Networks}
\subsubsection{Motivation}
The first approach is mainly based on the success of Tag Embeddings Recursive Neural Networks~\cite{tag-embedding-rnn}.
In this paper, the authors parameterized the composition functions at each parse tree's node with respect to the grammar rule expanding that node. 
By doing so, the authors was able to not only efficiently improve the performance of RNTN (Socher, 2013)~\cite{socher2013recursive} (from \(85.4\%\) to \(87.7\%\) accuracy) but also their models used much smaller number of parameters \((54K)\) compared to that of RNTN \((108K)\)~\cite{tag-embedding-rnn}.
Given the success of \hyperref[sec:treelstm]{Tree-LSTM models}, we hypothesized that by parameterized the composition functions of a Tree-LSTMs unit at a node with respect to the grammar rule expanding that node, we can improve the performance of Tree-LSTMs (in the same way how this method improving RNTN).

\subsubsection{Objectives}
\textbf{This approach was later proven to be unsuccessful}, so we tried two more different approaches.

\subsection{Transfer Learning by retraining Glove on Amazon Reviews dataset}

\subsubsection{Motivation}
There are many words (e.g. "B-rated", "Batman", "Nolan", "cartoonlike") which rarely appears in regular documents but more often in movie reviews.
We hypothesized that by training word embeddings of review documents, especially movie or book reviews, we can capture more rare words and also the different way people use words (or different word relationships) to express their opinions on movies or books.
For archiving these purposes, we retrained Glove vectors~\cite{glove} on part of  \hyperref[sec:amazon]{Amazon Reviews dataset}.
This new Glove vectors was named Glove Amazon.
For evaluating Glove Amazon, we replaced the Glove Common Crawl\footnote{Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) publicly available at \url{https://nlp.stanford.edu/projects/glove/}} with Glove Amazon for initializing Tree-LSTMs' word embedding layer.

\subsubsection{Objectives}
\textbf{In spite of being a simple method, it dramatically improves Tree-LSTMs performance.}
This is our first successful method for transfer learning from document-level (Amazon Reviews) to sentence-level labeled dataset (Stanford Sentiment Treebank).
Inspired by this method, we cooperated it into the development of our next approach.

\subsection{Combining Recusive Neural Networks with Convolution Neural Networks}

\subsubsection{Motivation}
Based on our discussion on tree-structured versus sequential network architects (Sec.\ref{sec:tree-discuss}) and the benefits of using convolution layer (Sec.\ref{kim-cnn}), we combined Convolution Neural Networks with Tree-LSTM and \hyperref[sec:lstm]{sequential LSTM}.
We hypothesized that the convolution layer will help Tree-LSTMs to mitigate the problem of lacking local context and weak feature capturing at leaf nodes (Sec.\ref{sec:tree-discuss}).
Additionally, using Tree-LSTM to combine the feature maps produced by convolution layer is better than max-over-time pooling layer (Sec.\ref{kim-drawback}).
The increased model complexity can lead to over-fitting.
We tackled this risk by unsupervised pre-training the models on the large Amazon Reviews dataset using methods described in Sec.\ref{sec:unsupervised-pretrain}.
Based on the success of Glove Amazon, we expected that the unsupervised pre-training process (on Amazon Reviews) help the models to capture not only generic language features but also specific knowledge about Film industry and human culture (Sec.\ref{sec:nlm}).

\subsubsection{Objectives}
With this approach, \textbf{we was able to archive state-of-art\footnote{July, 2017} performance on Stanford Sentiment Treebank}.


\section{Contributions}


\section{Structure of this thesis}
\begin{description}
\item [\deschyperlink{chap:background}{Chapter 2}] introduces some basic knowledge in NLP, Deep Learning and programming framework for implementing Deep Learning systems. 
\item [\deschyperlink{chap:related}{Chapter 3}] builds up the theoretical framework for the whole thesis. 
It analyzes the closely related works -- all the models or methods presented here are being used by at least one of our models.
For clarity, we divide Chapter 3 into two parts.
The first part is \hyperref[sec:dataset]{"Datasets"}. 
This part contains information about all datasets which were used in this thesis.
The second part is \hyperref[sec:composer]{"Neural network architects for sentence composition"}.
The models presented in this part are used to compose vector presentations of sentences.
These presentation vectors are then used to classify the sentiment class of the sentence.
For each model, we describe its structure, training method and evaluation on SST.
Additionally, we analyze advantages and disadvantages of each model compared to the others.
In this part, we also present and discuss several \hyperref[sec:unsupervised-pretrain]{unsupervised pre-train methods}.
\item [\deschyperlink{chap:method}{Chapter 4}] describes our new network architects, the reasons behind their designs, their training methods and other experiments. 
\item [\deschyperlink{chap:result}{Chapter 5}] presents, analyzes and discusses the empirical comparisons of our models and other related models.
\item [\deschyperlink{chap:conclude}{Chapter 6}] summarizes the achieved results, describes future works and conclusion.
\end{description}