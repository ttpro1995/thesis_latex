\hypertarget{chap:background}{\chapter{Background}}\label{chap:background}

\input{Background/feedforward}

\section{Parse tree in NLP}
Parse tree is a syntactic representation of a sentence. In this thesis, we work on two type of parse tree, Constituency Parse Tree, and Dependency Parse Tree.
\subsection{Constituency Parse Tree}
Constituency Parser breaks sentences into smaller phases. The root node represents a sentence. Children node represents a phrase or word constituting the parent node.
Each leaf node contains a word or punctuation. Each inner node contains a phrase. Each node is Part-of-speech tag labeled. Edges are unlabeled.

Constituency Parse Tree is constructed based on Chomsky Phase Structure~\cite{chomsky2002syntactic}. Given a list of rules, derive sentences following the rule. Constituency Parse Tree illustrates the derivation.


% \begin{equation}
% \label{eq:crule}
% \begin{aligned}
% &S \leftarrow NP + VP + .  \\
% &NP \leftarrow PRP  \\
% &NP \leftarrow DT + NN  \\
% &VP \leftarrow VBP + NP\\
% &VRP \leftarrow I \\
% &DT \leftarrow the \\
% &NN \leftarrow cat \\
% &VBP \leftarrow feed \\
% \end{aligned}
% \end{equation}
For example, we have the following rule:
\begin{enumerate}[label=(\roman*)]
    \item $S \leftarrow NP + VP + .$
    \item $NP \leftarrow PRP $
    \item $NP \leftarrow DT + NN$
    \item $VP \leftarrow VBP + NP$
    \item $VRP \leftarrow I$
    \item $DT \leftarrow the$
    \item $NN \leftarrow cat$
    \item $VBP \leftarrow feed$
\end{enumerate}
We apply rule to "I feed the cat". Result of derivation in Table \ref{ifeedmycat}  where Romans on right column indicate the rule used to derive previous statement. The derivation is illustrated in fig \ref{fig:ifeedthecatconstituency}

% \begin{equation}
% \label{eq:catparse}
% \begin{aligned}
% &S \\
% &NP + VP + .\\
% & PRP + VP + . \\
% & PRP + VBP + NP + . \\
% & PRP + VBP + DT + NN + . \\
% & I + VBP + DT + NN + . \\
% & I + feed + DT + NN + . \\
% & I + feed + my + NN + . \\
% & I + feed + my + cat + . \\
% \end{aligned}
% \end{equation}

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
	S	&  \\
	NP + VP + .	& (i) \\
	PRP + VP + .	& (ii) \\
	PRP + VBP + NP + .	& (iv)  \\
	PRP + VBP + DT + NN + .	& (iii) \\
	I + VBP + DT + NN + .	&  (v) \\
	I + feed + DT + NN + .	&  (viii) \\
	I + feed + the + NN + .	&  (vi) \\
	I + feed + the + cat + .	& (vii)
	\end{tabular}
\caption{I feed the cat derivation}
\label{ifeedmycat}
\end{table}

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.5\linewidth]{figure/ifeedthecatconstituency}
\begin{tikzpicture}
\node [sq] (v1) at (1,-1) {root};
\node [sq] (v2) at (1,-2) {S};
\node [sq] (v3) at (-1.5,-3) {NP};
\node [sq] (v6) at (-1.5,-4) {PRP};
\node [cir] (v7) at (-1.5,-5.5) {I};
\node [sq] (v4) at (1,-3) {VP};
\node [sq] (v8) at (0,-4) {VBP};
\node [cir] (v10) at (0,-5.5) {feed};
\node [cir] (v11) at (1.5,-5.5) {the};
\node [cir] (v12) at (3,-5.5) {cat};
\node [cir] (v13) at (4.5,-5.5) {.};
\node [sq] (v9) at (2,-4) {NP};
\node [sq] (v5) at (4.5,-3) {.};
\draw [->] (v1) edge (v2);
\draw [->] (v2) edge (v3);
\draw [->] (v2) edge (v4);
\draw [->] (v2) edge (v5);
\draw [->] (v3) edge (v6);
\draw [->] (v6) edge (v7);
\draw [->] (v4) edge (v8);
\draw [->] (v4) edge (v9);
\draw [->] (v8) edge (v10);
\draw [->] (v9) edge (v11);
\draw [->] (v9) edge (v12);
\draw [->] (v5) edge (v13);
\end{tikzpicture}
    \caption[Constituency Parse Tree]{Constituency Parse Tree}
    \label{fig:ifeedthecatconstituency}
\end{figure}

\subsection{Dependency Parse Tree}
Dependency Parse Tree represents dependency relationship of each word in sentences.
As opposed to Constituency Parse Tree, each node in Dependency Parse Tree contain one word.
Edges are labeled by type of dependency relationship of head word and the dependent. In this thesis, we use Universal Dependencies~\cite{nivre2016universal}. Fig \ref{fig:udexample} is an example of Dependency Parse Tree.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/udexample}
    \caption[Dependency Tree]{Dependency Tree}
    \label{fig:udexample}
\end{figure}


\section{Technical}
\subsection{Anaconda}
Anaconda\footnote{\url{https://www.continuum.io/downloads}} is a Python ecosystem for data science. Anaconda is an all-in-one installation coming with many advantages compared to traditional Python installation.

\subsubsection{All-in-one installation}
Anaconda is installed with essential packages for machine learning and data science such as Scipy\footnote{\url{https://www.scipy.org/}}, scikit-learn\footnote{\url{http://scikit-learn.org/}}, OpenBLAS\footnote{\url{http://www.openblas.net/}} and package manager tool otherwise have to install manually on traditional Python distro\footnote{\url{https://www.python.org/}}. All pre-install packages make installing Deep Learning framework easily. All-in-one installation is the most important feature that affects our choice of Anaconda.

\subsubsection{Package and environment manager}
In traditional Python, people use pip (preferred installer program)\footnote{\url{https://pypi.python.org/pypi/pip}} as package manager to install package and virtualenv\footnote{\url{https://virtualenv.pypa.io/en/stable/}}. Both have to be installed manually.

Anaconda distro comes with Conda\footnote{\url{https://conda.io/docs/}}, a package and environment manager.
Literally, Conda can replace both pip and virtualenv.
However, both Conda and pip co-exist because some Python package can only be installed from on pip, some others only exist on Conda.


\subsection{Deep Learning framework}
All Deep Learning Framework have some common features.
They are all capable of handling matrices computation efficiently.
Recently Deep Learning framework can leverage the power of GPU to run faster.
Each framework has their own backend implementation.
Thus, the benchmarks of different frameworks are variety.
The difference maybe insignificant in prototype, however .
We are making prototypes for Deep Learning models.
Therefore, documentation and language interface were our top criteria for choosing a framework.

\subsubsection{Torch}\label{sec:torch}
Torch\footnote{\url{http://torch.ch/}} is Lua scientific computing framework.
Torch support high performing matrix calculation via multi-dimensional array called Tensor. Torch is built with C/C++, CUDA backend.
Torch's authors chose Lua because Lua works well with C/C++~\cite{collobert2011torch7}.  Thus, Torch is high performing and support GPU. Torch have neural network package (nn) package. Computational graph must be defined before forward pass.

A simple, single linear layer network can be easily defined with few line of code (see listing \ref{lst:torchlinear}).

\begin{lstlisting}[caption={Simple linear layer in Torch},label={lst:torchlinear}, language={[5.1]Lua}]
-- simple y = Ax + b linear layer
l = nn.Linear(2,3)
-- forward pass
x = torch.Tensor(2)
y = l:forward(x) -- vector dimension of 3
\end{lstlisting}

However, when a model needs multiple modules, such as multilayer perceptron (MLP), these modules must be put into container. Figure \ref{fig:nncontainer} illustrates on function of each nn container. To construct two-layer perception (Eq.\ref{eq:mlp}), linear, tanh and softmax module must be packed into sequential module (see listing \ref{lst:torchmlp}).

\begin{equation}
\label{eq:mlp}
\begin{aligned}
&h = tanh(W_1*x + b_1) \\
&y = softmax(W_2*h + b2)
\end{aligned}
\end{equation}


\begin{lstlisting}[caption={MLP in Torch},label={lst:torchmlp}, language={[5.1]Lua}]
model = nn.Sequential()
model:add(nn.Linear(2,3))
model:add(nn.Tanh())
model:add(nn.Linear(3,5))
model:add(nn.SoftMax())
-- forward
x = torch.Tensor(2)
y = model:forward(x)
\end{lstlisting}

Torch provides nngraph package to support building more complicated models. For example, define MLP in (Eq.\ref{eq:mlp}) using nngraph (see listing \ref{lst:torchnngraph})

\begin{lstlisting}[caption={MLP using nngraph},label={lst:torchnngraph}, language={[5.1]Lua}]
model = nn.Sequential()
model:add(nn.Linear(2,3))
model:add(nn.Tanh())
model:add(nn.Linear(3,5))
model:add(nn.SoftMax())
-- forward
x = torch.Tensor(2)
y = model:forward(x)
\end{lstlisting}

Sample code on training a model, see Appendix \ref{lst:torchtrain}

\subsubsection{Theano}
Theano\footnote{\url{http://deeplearning.net/software/theano/}} is a deep learning library on Python. Its basic function is similar to Torch: matrix calculation, support GPU. Theano is define-and-run scheme, on which a computation graph must be built before executed.

\begin{lstlisting}[caption={Define function in Theano},label={lst:theanof}, language={python}]
x = T.dmatrix('x')
y = T.dmatrix('y')
z = x + y
f = function([x, y], z)
f([[1, 1], [2, 2]], [[3, 3], [4, 4]])
# result [[4, 4], [6, 6]]
\end{lstlisting}

Comparing to Torch7, Theano is slower on most benchmark~\cite{collobert2011torch7}. Theano does not provide nice template like linear layer. Thus, the model must be defined from equation. It gives researcher more control over mathematics aspect but causes more trouble for beginner. A sample code for MLP \ref{lst:theanomlp}. One more problem is that the 'define-and-run' scheme does not suitable for recursive neural network due to recompiling the computation graph each training sample take time.

There is a library called Keras~\cite{chollet2017keras}, a high-level deep learning interface that works on top of Theano, TensorFlow, CNTK. Keras make mathematics in Theano simple for beginners and convenient for experienced users.

\subsubsection{Pytorch}\label{sec:pytorch}
PyTorch uses the same backend as Torch. However, PyTorch is specially designed for Python. Pytorch has pre-defined modules (Linear layer, Convolution layer) like Torch. However, Pytorch does not require to pack model into container. In Pytorch networks are defined in forward-pass thanks to Dynamic Neural Networks feature. Therefore, user can use Python control flow to define a network. For example, one can use for loop to run recurrent neural network (see listing \ref{lst:pytorchrnn}) .
This feature allows us to implement Recursive Neural Network for NLP, of which the network change for every sample, much more easier.

\begin{lstlisting}[caption={RNN},label={lst:pytorchrnn}, language={python}]
import torch
import torch.nn as nn
rnn = nn.RNNCell(10, 20)
seq_len = 10
input_dim = 100
hidden_dim = 150
input = Variable(torch.randn(seq_len, 1, input_dim))
hx = Variable(torch.zeros(1, hidden_dim))
output = []
for i in range(6):
    hx = rnn(input[i], hx)
    output.append(hx)
\end{lstlisting}

We also implement treelstm from original Torch7\footnote{\url{https://github.com/stanfordnlp/treelstm}} sentiment classification task in PyTorch and publish on Github\footnote{\url{https://github.com/ttpro1995/TreeLSTMSentiment}}.

We choose PyTorch because:
\begin{itemize}
    \item Dynamic Neural Networks' feature works well on data sequence with different length
    \item Well-written documentation
    \item Intuitive framework
    \item Easy to install and run on CUDA
\end{itemize}
