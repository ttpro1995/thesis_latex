\hypertarget{chap:result}{\chapter{Experimental setups and results}}\label{result-discuss}
\section{Datasets}
\subsection{Stanford Sentiment Treebank}
In this thesis, we used Standford Sentiment Treebank (SST) dataset~\cite{socher2013recursive} to evaluate our models sentence-level sentiment analysis task.
Originally, the dataset was built based on Rotten Tomatoes Movie Review dataset~\cite{Rotten-Tomato}~\cite{socher2013recursive}.
In total, Standford Sentiment Treebank contains 11,855 sentences.
The dataset was split into training, validation and testing set which contains 8544, 1101 and 2210 sentences respectively (Fig.\ref{fig:sstfinegrain_5}).
Table \ref{table:sststatistic_5} shows the distribution of sentiment classes on training, validation and testing set.

In this dataset, every sentence was parsed using Stanford (constituency) parser~\cite{socher2013recursive}\footnote{Open source tool: \url{https://stanfordnlp.github.io/CoreNLP/}}, each phrase which is spanned by any sub-tree of the parse tree is then labeled with  a fine-grained sentiment label (see Fig.\ref{fig:sst_5}).
Fine-grained sentiment is a setting which partition all sentiments into 5 classes: "Positive", "Somewhat Positive", "Neutral", "Somewhat Negative" and "Negative"~\cite{socher2013recursive} (a.k.a. \textbf{Fine-grained setting}).
There are total 215,154 phrases in the whole dataset.

For \textbf{Binary setting}, there are only to sentiment classed on the entire dataset, which means all neutral sentiment sentences are removed, "Positive" and "Somewhat Positive" are merged into one class, the same rule applied on "Somewhat Negative" and "Negative"~\cite{socher2013recursive}.
After all "Neutral" sentences have been removed, there are 6920/872/1821 sentences remained in train/dev/test set (Fig.\ref{fig:sstbinary_5}).


Table \ref{table:sststatistic_5} show statistics by sentences of Sentiment Treebank Dataset.
SST dataset is publicly available online\footnote{https://nlp.stanford.edu/sentiment/index.html}.

\begin{figure}[H]
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figure/sst}
        \caption[A parsed sentence in SST]{A parsed sentence in SST\footnote{Render by Pytreebank \url{https://github.com/JonathanRaiman/pytreebank}}}
        \label{fig:sst_5}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figure/sstfinegrain}
        \caption[SST fine-grain sentiment distribution]{SST fine-grain sentiment distribution}
        \label{fig:sstfinegrain_5}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figure/sstbinary}
        \caption[SST binary sentiment distribution]{SST binary sentiment distribution}
        \label{fig:sstbinary_5}
    \end{minipage}
\end{figure}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
    \centering
    \caption{SST statistics}
    \label{table:sststatistic_5}
    \begin{tabular}{lllll}
        Number of review       &               &      &                       &  \\ \cline{1-4}
        \multirow{5}{*}{Train} & Very Negative & 1092 & \multirow{2}{*}{3310} &  \\ \cline{2-3}
        & Negative      & 2218 &                       &  \\ \cline{2-4}
        & Neutral       & 1624 &                       &  \\ \cline{2-4}
        & Positive      & 2322 & \multirow{2}{*}{3610} &  \\ \cline{2-3}
        & Very positive & 1288 &                       &  \\ \cline{1-4}
        \multirow{5}{*}{Dev}   & Very Negative & 139  & \multirow{2}{*}{428}  &  \\ \cline{2-3}
        & Negative      & 289  &                       &  \\ \cline{2-4}
        & Neutral       & 229  &                       &  \\ \cline{2-4}
        & Positive      & 279  & \multirow{2}{*}{444}  &  \\ \cline{2-3}
        & Very positive & 165  &                       &  \\ \cline{1-4}
        \multirow{5}{*}{Test}  & Very Negative & 279  & \multirow{2}{*}{912}  &  \\ \cline{2-3}
        & Negative      & 633  &                       &  \\ \cline{2-4}
        & Neutral       & 389  &                       &  \\ \cline{2-4}
        & Positive      & 510  & \multirow{2}{*}{909}  &  \\ \cline{2-3}
        & Very Positive & 399  &                       &  \\ \cline{2-4}
    \end{tabular}
\end{table}
\subsection{Amazon Reviews}
Amazon Reviews is a gigantic review dataset
which contains 142.8 million reviews from Amazon spanning May 1996 - July 2014\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}.
Each review contains product review (rating, text, helpfulness vote) and metadata (descriptions, category information, price, brand, and image features)~\cite{amazon-reviews}.
In this thesis, we only used a small part of Amazon Reviews.
These parts include Amazon Movies and TV reviews (7,850,072 reviews)~\cite{mcauley2013hidden}, Amazon Book reviews (22,507,155 reviews) and the new Movies and TV reviews (4,607,047 reviews)~\cite{McAuleyTSH15}~\cite{HeM16}.

Listing \ref{lst:amzreview_5} is a sample of book reviews.
Amazon Book reviews and new Movies and TV dataset have the same format. Listing \ref{lst:oldamzreview_5} is a sample of Amazon Movies and TV reviews old dataset (7,850,072 reviews).

\begin{lstlisting}[caption={Amazon reviews sample},label={lst:amzreview_5}]
    {
        "reviewerID": "AH2L9G3DQHHAJ",
        "asin": "0000000116",
        "reviewerName": "chris",
        "helpful": [5, 5],
        "reviewText": "Interesting Grisham tale of a lawyer that takes millions of dollars from his firm after faking his own death. Grisham usually is able to hook his readers early and ,in this case, doesn't play his hand to soon. The usually reliable Frank Mueller makes this story even an even better bet on Audiobook.",
        "overall": 4.0,
        "summary": "Show me the money!",
        "unixReviewTime": 1019865600,
        "reviewTime": "04 27, 2002"
    }
\end{lstlisting}

\begin{lstlisting}[caption={Old Amazon reviews sample},label={lst:oldamzreview_5}]
{
"reviewerID": "AH2L9G3DQHHAJ",
"asin": "0000000116",
"reviewerName": "chris",
"helpful": [5, 5],
"reviewText": "Interesting Grisham tale of a lawyer that takes millions of dollars from his firm after faking his own death. Grisham usually is able to hook his readers early and ,in this case, doesn't play his hand to soon. The usually reliable Frank Mueller makes this story even an even better bet on Audiobook.",
"overall": 4.0,
"summary": "Show me the money!",
"unixReviewTime": 1019865600,
"reviewTime": "04 27, 2002"
}
\end{lstlisting}

The differences between the old Amazon reviews dataset\footnote{\url{https://snap.stanford.edu/data/web-Amazon.html}} and new Amazon reviews dataset\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}} is that:
They were gathered using different crawling methods, the author also solved the duplication problem in new the dataset~\cite{amazon-reviews}.
Table \ref{table:moviereview_5} shows number of reviews by overall.

\begin{table}[H]
    \centering
    \caption{New Movies and TV dataset}
    \label{table:moviereview_5}
    \begin{tabular}{@{}lllc@{}}
        \toprule
        & \multicolumn{3}{l}{Number of reviews}                         \\ \midrule
        5-Star & 2761408 & \multirow{2}{*}{3618913} & \multirow{5}{*}{4607047} \\ \cmidrule(r){1-2}
        4-Star & 857505  &                          &                          \\ \cmidrule(r){1-3}
        3-Star & \multicolumn{2}{l}{415369}         &                          \\ \cmidrule(r){1-3}
        2-Star & 233221  & \multirow{2}{*}{572765}  &                          \\ \cmidrule(r){1-2}
        1-Star & 339544  &                          &                          \\ \bottomrule
    \end{tabular}
\end{table}

\section{Experiment setups}
\subsection{Utilizing local syntactic information at each node of Recursive Neural Networks}
\subsubsection{Preprocessing}
\subsubsection{Hyper-parameters and training}
\subsection{Transfer Learning by retraining Glove on Amazon Reviews dataset}
\subsubsection{Preprocessing}
\subsubsection{Hyper-parameters and training}
\subsection{Combining Recursive Neural Networks with Convolution Neural Networks}
\subsubsection{Preprocessing}
\subsubsection{Hyper-parameters and training}
\section{Results and discussion}
Experiment results are summaries in Table \ref{table:experimentresult}. In all of our models, we updated word vectors manually (eq. \ref{eq:updated}).
Empirically, this helped us to obtain better results than using Adam, Adagrad or Adadelta.

% \begin{table}[H]
%     \centering
%     \caption{Experiment result. For our experiment, we report mean accuracies of 5 runs. Max value in bracket.}
%     \label{table:experimentresult}
%     \begin{tabular}{ll}
%         Method                                   & Binary \\ \hline
%         LSTM                                     & 86.40   \\
%         BiLSTM                                   & 85.80   \\ \hline
%         Constituency Tree-LSTM ~\cite{treeLSTM} & 88.00     \\
%         Constituency Tree-LSTM ~\cite{treeLSTM} (Glove Amazon) & 88.85 (89.35) \\
%         Dependency Tree-LSTM  ~\cite{treeLSTM}  & 85.70   \\
%         CNN GRU ~\cite{cnn-rnn}                    & 89.13(89.95) *    \\
%         CNN LSTM ~\cite{cnn-rnn}                    & 89.43(89.56) *    \\ \hline
%         Constituency TE Tree-GRU                 & 87.65 (88.25)  \\
%         Dependency TE Tree-GRU                   & 87.13  (88)  \\ \hline
%         CNN LSTM                                 & 89.10 (89.40)      \\
%         % CNN LSTM (pretrained)**                 &            \\
%         2 Channel CNN LSTM                        & 89.54    (89.79)    \\
%         % 2 Channel CNN LSTM (pretrained)**        &         \\
%         CNN Tree-LSTM                            & 88.82 (88.92) \\
%         CNN Tree-LSTM (Glove Amazon)             & 88.96 (89.18) \\
%         2 Channel CNN Tree-LSTM                  & 89.66 (90.12)
%     \end{tabular}
% \end{table}

\begin{table}[H]
    \centering
    \caption[Experiment result on SST]{Experiment results of models evaluated on Stanford Sentiment Treebank with binary setting.
The models which have both data of mean(std) and max are models which have been evaluated by us.
For these models, we report mean, standard deviation and max of 5 runs.
If a model has data of only mean(std) or only max, the data was taken from its originated research paper.
If the data of both mean(std) and max is missing, the model was not evaluated yet.}
    \label{table:experimentresult}
    \begin{tabular}{c|lll}
    \textbf{Block}    & \textbf{Model}  & \textbf{Mean(std)} & \textbf{Max}   \\
\Xhline{3\arrayrulewidth}
\Xhline{3\arrayrulewidth}

    \multirow{4}{*}{A} & CNN-non-static~\cite{KimCNN} & - & 87.20\Tstrut \\
        & CNN-multichannel~\cite{KimCNN} & - & 88.10 \\
    & DCNN~\cite{DCNN} & - & 86.80 \\
    & MVCNN~\cite{2-layer-cnn} & - & 89.40 \\
\hline
        \multirow{5}{*}{B} & LSTM~\cite{originLSTM}    & 86.64 (0.27) & 86.93  \\
        & BiLSTM~\cite{GravesLSTM}  & 85.80 (0.69) & 86.43   \\
        & 2-layer LSTM~\cite{GravesLSTM} & 86.30 (0.60) & - \\
         & 2-layer Bidirectional LSTM~\cite{GravesLSTM} & 87.20 (1.00) & - \\
         & DMN~\cite{attention-gru} & - & 88.60 \\
\hline
        \multirow{5}{*}{C} & RNTN~\cite{socher2013recursive}  & - & 85.40  \\
        & DRNN~\cite{IrsoyDRNN} & - & 86.60 \\
        & TE-RNTN~\cite{tag-embedding-rnn} & - & 87.70 \\
        & Dependency Tree-LSTM  ~\cite{treeLSTM}  & 85.70 (0.30)  & 85.80 \\
         & Constituency Tree-LSTM ~\cite{treeLSTM} & 88.00 (0.40)    &   88.19\\
\hline
        \multirow{3}{*}{D} & GICF~\cite{group-instance} & - & 85.70 \\
         & Paragraph-Vec~\cite{ParagraphVec} & - & 87.80 \\
         & LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} & 87.98 (0.46) & 88.50 (89.20)*
          \\
\hline
         \multirow{2}{*}{E}  & CNN GRU ~\cite{cnn-rnn}                    & 89.13 (0.29)  &  89.61 (89.95)*    \\
         & CNN LSTM ~\cite{cnn-rnn}                    & 89.43 (0.28)  & 89.72 (89.56)*\Bstrut    \\
\Xhline{3\arrayrulewidth}
\Xhline{3\arrayrulewidth}
         \multirow{2}{*}{F} & Constituency TE Tree-GRU                 & 87.65 (0.34) & 88.25\Tstrut \\
          & Dependency TE Tree-GRU                   & 87.13 (0.70)  & 88.00\Bstrut \\
\hline
\hline
        \multirow{1}{*}{G} & Constituency Tree-LSTM ~\cite{treeLSTM} (Glove Amazon) & 88.85 (0.44) & 89.35\Tstrut\Bstrut \\
\hline
\hline
        \multirow{3}{*}{H} & CNN LSTM                                 & 89.10 (0.39)  & 89.40 \Tstrut  \\
        & 2 Channel CNN LSTM                        & 89.54    (0.22) & 89.79    \\
        & Multichannel CNN LSTM (pretrained) & - & - \\
\hline
        \multirow{4}{*}{I} & CNN Tree-LSTM                            & 88.82 (0.13) & 88.92 \\
        & CNN Tree-LSTM (Glove Amazon)             & 88.96 (0.24) & 89.18 \\
        & 2 Channel CNN Tree-LSTM  &\textbf{89.69 (0.36)} & \textbf{90.12}    \\
        & Multichannel CNN Tree-LSTM (pretrained)        & - & -        \\
    \end{tabular}
\end{table}

\textit{(*): Result reported by the original paper.}

% \textit{(***): CNN and LSTM layer are initialize using pre-trained parameters from Section \ref{sec:CNNtree}}

Table \ref{table:experimentresult} is divided into two parts.
The first part is from Block A to E, which contains all baselines model.
The second part is from Block F to I, which contains all models which were proposed and evaluated by us.

\textbf{Descriptions of each Block in the first part}:
\begin{description}
\item[Block A] contains Multilayer Convolution Neural Networks models.
CNN-non-static and CNN-multichannel~\cite{KimCNN} are single layer CNN (Sec.\ref{kim-cnn}).
DCNN~\cite{DCNN} and MVCNN~\cite{2-layer-cnn} are multilayer CNN, with MVCNN is a very large model  which has 2 layers, 5 word embeddings channels and unsupervised pre-train using the method of Sentence Encoding (Sec.\ref{sec:2-layer-cnn})
\item[Block B] contains sequential/recurrent models.
 LSTM~\cite{originLSTM}, BiLSTM~\cite{GravesLSTM}, 2-layer LSTM~\cite{GravesLSTM} and 2-layer Bidirectional LSTM~\cite{GravesLSTM} have been described in Sec.\ref{sec:RNN}.
DMN~\cite{attention-gru} is a sophisticated model used GRU with attention mechanism and episodic memory.
\item[Block C] contains models which belong to the family of Recursive Neural Networks (tree-structured model).
RNTN~\cite{socher2013recursive} is the first recursive neural network to successfully apply on sentence-level sentiment analysis (Stanford Sentiment Treebank).
It was inspired by the idea that natural languages have recursive structure, to understand a sentence we must understand its phrases and, to understand a phrase, we must understand its words.
DRNN~\cite{IrsoyDRNN} is a multilayered extension of RNTN.
TE-RNTN is also an extension of RNTN which utilize the local syntactic information at each node of a sentence's parse tree.
Dependency and Constituency Tree-LSTM~\cite{treeLSTM} are tree-structured versions of LSTM which have been described in Sec.\ref{sec:treelstm}.

\item[Block D] contains transfer learning methods, which utilized a large amount of data other than Stanford Sentiment Treebank.
GICF~\cite{group-instance} is an attempt to learn to classify sentiments of sentences (in Stanford Sentiment Treebank) from training dataset which contains only document-level sentiment labels.
Paragraph-Vec~\cite{ParagraphVec} is a method that learns to encode any sequence of words into a vector with the purpose of maximizing the likelihood of words which appear in that sequence given the encoding vector.
LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} is a LSTM models with word embeddings layer initialized by PARAGRAM-SL999, word vectors trained on a large paraphrase dataset (PPDB~\cite{ganitkevitch2013ppdb}) % which was trained using a large paraphrase dataset (PPDB~\cite{ganitkevitch2013ppdb}).
In our experiments with LSTM (PARAGRAM-SL999), we used the implementation and pre-trained word vectors which are publicly available on the author website\footnote{\url{http://ttic.uchicago.edu/~wieting/}}.

\item[Block E] contains models which combine Convolution Neural Networks and Recurrent Neural Networks.
CNN GRU and CNN LSTM~\cite{cnn-rnn} have been described in detail in Sec.\ref{cnn-rnn}.
In our experiments with CNN GRU and CNN LSTM, we used the implementation publicly published by the authors\footnote{\url{https://github.com/ultimate010/crnn}}.
\end{description}

\textbf{Descriptions of each Block in the second part}:
\begin{description}
\item[Block F] contains models which follow our approach of utilizing local syntactic information at each node of Recursive Neural Networks (Sec.\ref{sec:VTtree})
\item[Block G] contains the experiment which follows our approach of applying Transfer Learning by retraining Glove on Amazon Reviews dataset (Sec.\ref{sec:gloveamazone})
\item[Block H and I] contain the models which follow our approach of combining Recursive Neural Networks with Convolution Neural Networks (Sec.\ref{sec:CNNtree}).
Block H is for sequential architects, while Block I is for tree-structured architects.
\end{description}

\subsection{Utilizing local syntactic information at each node of Recursive Neural Networks}
Our dependency TE Tree-GRU outperformed Dependency Tree-LSTM when using the same dependency parsed dataset.
But disappointedly, TE Tree-GRU underperformed Dependency Tree-LSTM on Semantic Relatedness task (SemEval 2014, Task 1~\cite{SemeEvalTask1}) which pointed out the strength of Dependency Tree-LSTM and the main reason why it was presented in the original paper~\cite{treeLSTM}.
Constituency TE Tree-GRU model is slightly better than Dependency TE Tree-GRU.
This performance gap is expected because Dependency parse tree has less labeled node comparing to Constituency parse tree~\cite{treeLSTM}.
We discovered that training model composition with Adam learning rate of 0.001 and word representation (updated manually) learning rate at 0.05 for 20 epochs yield the best result.
Training more epochs does not improve the performing.

\label{unproved:tag-useless}
On overall, this approach does not improving Tree-LSTMs, it might be because Tree-LSTMs have already utilized the information in word embeddings and the local syntactic information from tag embeddings added no more value.


\subsection{Transfer Learning by retraining Glove on Amazon Reviews dataset}
\label{fact:glove-amazon-improve-tree}
In Block G, Constituency Tree-LSTM using Glove Amazon largely outperformed Constituency Tree-LSTM using Glove Common Crawl even though it was trained on significantly larger dataset (840B tokens) compared to our preprocessed Amazon Reviews (4.7B tokens) (Sec.\ref{sec:gloveamazone}).
This method also outperformed all the Transfer Learning methods in Block D.
Compared to Glove Amazon method, originally,  Paragraph-Vec~\cite{ParagraphVec} cannot be fine-tuning in a supervised manner.
We also used PARAGRAM-SL999 for initializing the word embeddings layer of Constituency Tree-LSTM,
the results (of 8 runs with mean 87.175\% and standard deviation 0.69) were not as good as those of Glove Common Crawl.

These results support our hypothesis that by training word embeddings on review documents, especially movie or book reviews, we can capture more rare words and also the different way people use words (or different word relationships) when they express their opinions on movies or books (Sec.\ref{movie-hypothesis}).

\subsection{Combining Recursive Neural Networks with Convolution Neural Networks}
\label{proved:tree-conv-benefit}
The fact that CNN Tree-LSTM outperforms Constituency Tree-LSTM~\cite{treeLSTM} supports our hypothesis on the benefits of combining convolution layers with Tree-LSTM (Sec.\ref{conv-tree-benefits}).
Due to using similar architects, CNN LSTM was comparable with CNN GRU and CNN LSTM~\cite{cnn-rnn}. \label{unproved:cnn-treelstm-overfit}
CNN Tree-LSTM performed worst than CNN LSTM, the reason might because of over-fitting.
We will prove this hypothesis by looking at the plots of error rate on training and validation set of these two models.

\label{proved:Amazon-adv-Common}
We have seen that Glove Amazon improved Tree-LSTM in Sec.\ref{fact:glove-amazon-improve-tree}.
In these experiments (Block I), Glove Amazon also improved CNN Tree-LSTM.
We can conclude that Amazon Glove captured some good\footnote{good for the task of sentiment analysis of movie reviews} features that does not exist or hardly be extracted in Glove Common Crawl.

According to Table.\ref{table:paramtable}, the number of parameters of 2 Channel CNN Tree-LSTM (722,153) is much larger than that of CNN Tree-LSTM (482,153), which makes it more likely for 2 Channel CNN Tree-LSTM to over-fit the training data.
However, in fact, 2 Channel CNN Tree-LSTM was able to archive much higher accuracy than both CNN Tree-LSTM (Glove Amazon) and CNN Tree-LSTM (Glove Common Crawl).\label{proved:Common-syn-Amazon}
The result proves that there are some features only appearing in Glove Common Crawl or when combining both Glove Amazon and Glove Common Crawl.
Otherwise, there would not be any improvement.

At the time of this submission\footnote{20/07/2017}, we have not finished our experiments on unsupervised pre-training methods (Sec.\ref{enhan-unsupervised-pretrain}), we will add the results of these experiments as soon as possible.

We found that 100 filters of size 3 words and 100 filters of size 5 words yield better results compared to single filters size or the number of filters larger than 200. We trained with Adagrad of learning rate of 0.01 and word vectors (updated manually) at learning rate 0.1 give the best result. We trained for 60 epochs.
