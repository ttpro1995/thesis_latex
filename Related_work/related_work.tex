\chapter{Related Works}

\section{Data sets}
\subsection{Stanford Sentiment Treebank} \label{sec:sst}
In this thesis, we use Standford Sentiment Treebank (SST) dataset~\cite{socher2013recursive}. The dataset was original collected from \url{https://www.rottentomatoes.com/} and publish in \cite{pang2002thumbs}. Standford Sentiment Treebank contains 11,855 sentences. Each data sentence consist of fined-grain sentiment labeled phrases in constituency parse tree structure (see \textbf{Figure \ref{fig:sst}}). There are total 215,154 phrases in whole dataset. 
The dataset was splitted into train/dev/test contain 8544/1101/2210 sentences each for training and evaluation models. After remove neutral sentiment sentences, there are 6920/872/1821 sentences remained in train/dev/test set.

SST dataset are publicly available online \footnote{https://nlp.stanford.edu/sentiment/index.html}.

\begin{figure}[H]
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figure/sst}
		\caption[A parsed sentence in SST]{A parsed sentence in SST \footnote{Render by Pytreebank \url{https://github.com/JonathanRaiman/pytreebank}}}
		\label{fig:sst}
	\end{minipage}
\end{figure}

\footnote{\url{https://github.com/stanfordnlp/treelstm}} to preprocess SST.

\subsection{Julian McAuley's Amazon reviews data set}
We get Amazon Movies and TV reviews dataset (7,850,072 reviews) \cite{mcauley2013hidden}, Amazon Book reviews (22,507,155 reviews) and new Movies and TV dataset (4,607,047 reviews) \cite{he2016ups}. 

Listing \ref{lst:amzreview} is sample of one book review. Amazon Book reviews and new Movies and TV dataset  have same format. Listing \ref{lst:oldamzreview} is sample of Amazon Movies and TV reviews old dataset (7,850,072 reviews). 

\begin{lstlisting}[caption={Amazon reviews sample},label={lst:amzreview}]
	{
		"reviewerID": "AH2L9G3DQHHAJ",
		"asin": "0000000116",
		"reviewerName": "chris",
		"helpful": [5, 5],
		"reviewText": "Interesting Grisham tale of a lawyer that takes millions of dollars from his firm after faking his own death. Grisham usually is able to hook his readers early and ,in this case, doesn't play his hand to soon. The usually reliable Frank Mueller makes this story even an even better bet on Audiobook.",
		"overall": 4.0,
		"summary": "Show me the money!",
		"unixReviewTime": 1019865600,
		"reviewTime": "04 27, 2002"
	}
\end{lstlisting}

\begin{lstlisting}[caption={Old Amazon reviews sample},label={lst:oldamzreview}]
{
"reviewerID": "AH2L9G3DQHHAJ",
"asin": "0000000116",
"reviewerName": "chris",
"helpful": [5, 5],
"reviewText": "Interesting Grisham tale of a lawyer that takes millions of dollars from his firm after faking his own death. Grisham usually is able to hook his readers early and ,in this case, doesn't play his hand to soon. The usually reliable Frank Mueller makes this story even an even better bet on Audiobook.",
"overall": 4.0,
"summary": "Show me the money!",
"unixReviewTime": 1019865600,
"reviewTime": "04 27, 2002"
}
\end{lstlisting}

According to old Web data: Amazon reviews website  \footnote{\url{https://snap.stanford.edu/data/web-Amazon.html}} and new Amazon product data \footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}, the author states that he have solve the duplication problem in new dataset. We decide to use old Amazon Movies and TV reviews combined Amazon book reviews for unsupervised training Glove and new Amazon Movies and TV reviews dataset for supervised training in section \ref{sec:improveembedding}. Table \ref{table:moviereview} show number of reviews by overall.

\begin{table}[H]
	\centering
	\caption{New Movies and TV dataset}
	\label{table:moviereview}
	\begin{tabular}{@{}lllc@{}}
		\toprule
		& \multicolumn{3}{l}{Number of reviews}                         \\ \midrule
		5-Star & 2761408 & \multirow{2}{*}{3618913} & \multirow{5}{*}{4607047} \\ \cmidrule(r){1-2}
		4-Star & 857505  &                          &                          \\ \cmidrule(r){1-3}
		3-Star & \multicolumn{2}{l}{415369}         &                          \\ \cmidrule(r){1-3}
		2-Star & 233221  & \multirow{2}{*}{572765}  &                          \\ \cmidrule(r){1-2}
		1-Star & 339544  &                          &                          \\ \bottomrule
	\end{tabular}
\end{table}


\section{Improving sentence composition}


\subsection{Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}
Compare to other network architects, Recurrent Neural Network have a great advantage in handling sequential, arbitrary length input (e.g. sentences, paragraphs, a sequence of frame in a video or a sequence of acoustic unit in a spoken word) but on the other hand, it also come with the disadvantage of being hard to train~\cite{hardRNN}.
Given a sequence of input, the network has to capture features which can be long-range relationships between inputs~\cite{socher2013recursive}.
One way to improve such structure would be to make these features easier to capture, in other words, shorten the length of relationships between inputs.
For the case of sentence, we can present the it as a syntactic parse tree, in which relevant words and phrases are presented closer to each other in a intuitive way (as human, we understand sentences base on phased and phrases base on words). 
In this paper~\cite{treeLSTM}, the authors explored this idea by using tree-structured LSTM to combine sentence presentation from its words.
They archive state-of-the-art performance on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1~\cite{SemeEvalTask1}) and sentiment classification (Stanford Sentiment Treebank~\cite{socher2013recursive}).


Given the advantages of tree structure over it sequential counterpart, we will apply this theory into our model in \hyperref[sec:VTtree]{4.1.1} and \hyperref[sec:CNNtree]{4.1.2}. 
Their experiments were originally implemented in \hyperref[sec:torch]{Torch7}~\footnote{https://github.com/stanfordnlp/treelstm}, for the purpose of extending their models and doing more experiments, we re-implemented~\footnote{https://github.com/ttpro1995/TreeLSTMSentiment} their models in \hyperref[sec:pytorch]{PyTorch}. 

As we only interested in  the task of sentiment analysis, the experiment and result which related to the task of semantic relatedness will not be presented.

\subsubsection{Method}
\paragraph{Child-Sum Tree-LSTMs}
Given a tree, we denote \(C(j)\) as set of children of node \(j\).
The step of calculation inside Child-Sum Tree-LSTM node \(j\) can be expressed as follow~\cite{treeLSTM}:
\begin{align}
  	\tilde{h_j} &= \sum_{k \in C(j)} h_k &\label{eq1:2}\\
  	i_j &= \sigma{(W^{(i)}x_j + U^{(i)}\tilde{h_j} + b^{(i)})} &\label{eq1:3}\\
  	f_{jk} &= \sigma{(W^{(f)}x_j + U^{(i)}h_k + b^{(f)})}, \qquad  \forall k \in C(j) & \label{eq1:foget1}\\
  	o_j &= \sigma{(W^{(o)}x_j + U^{(o)}\tilde{h_j} + b^{(o)})} &\label{eq1:5}\\
  	u_j &= \tanh{(W^{(u)}x_j + U^{(u)}\tilde{h_j} + b^{(u)})} &\label{eq1:6}\\
   	c_j &= i_j \odot u_j + \sum_{k \in C(j)} f_{jk} \odot c_k & \\
	h_j &= o_j \odot \tanh{(c_j)} &
\end{align}

As Child-Sum Tree-LSTMs has ability to combine node with arbitrary number of children, it is can be applied to compose sentence bases on it dependency parse tree.
This application was named by the authors as \textbf{Dependency Tree-LSTMs}~\cite{treeLSTM}.

\paragraph{N-ary Tree-LSTMs}
Given a tree, we denote \(C(j)\) as set of children of node \(j\) and \(N\) as the maximum number of children a node can have. 
We will assume that the number of child in a node is always \(0\) or \(N\). 
If a node have no children nodes, we call it a leaf-node, else, it will be called a composer-node. 

The step of calculation inside leaf-node \(j\) can be expressed as follow:
\begin{align}
	o_j &= \sigma{\left( W^{(o)} x_j + a^{\left(o\right)}\right)} & \\
   	c_j &= W^{(c)} x_j + a^{(c)} & \\
	h_j &= o_j \odot \tanh{\left(c_j\right)} &
\end{align}

The step of calculation inside composer-node \(j\) can be expressed as follow:
\begin{align}
  	i_j &= \sigma{ \left(\sum_{l=1}^{N}U_l^{(i)} h_{jl} + b^{(i)} \right) } &\label{eq:9}\\
  	f_{jk} &= \sigma{\left(\sum_{l=1}^{N}U_{kl}^{\left(f\right)} h_{jl} + b^{\left(f\right)}\right)}, \qquad  \forall k \in C(j) & \label{eq:foget2}\\ 
  	o_j &= \sigma{\left( \sum_{l=1}^{N}U_l^{\left(o\right)} h_{jl} + b^{\left(o\right)}\right)} &\label{eq:11}\\
  	u_j &= \tanh{\left( \sum_{l=1}^{N}U_l^{\left(u\right)} h_{jl} + b^{\left(u\right)}\right)} &\label{eq:12}\\
   	c_j &= i_j \odot u_j + \sum_{k \in C\left(j\right)} f_{jk} \odot c_{jl} & \\
	h_j &= o_j \odot \tanh{\left(c_j\right)} & \\
\end{align}

Different from Child-Sum Tree-LSTMs forget gate in Eq.\eqref{eq1:foget1}, N-ary Tree-LSTMs chooses what to forget based on all it children as in Eq.\eqref{eq:foget2}. 
Also, each the combination of \(h_{jl}\) in Eqs.\eqref{eq:9},\eqref{eq:11}, \eqref{eq:12} are parameterize by \(U_l^{(i)}\), \(U_l^{(o)}\) and \(U_l^{(u)}\) respectively, compares to linear transformation of sum (Eqs.\eqref{eq1:2}, \eqref{eq1:3}, \eqref{eq1:5}, \eqref{eq1:6}) in Child-Sum Tree-LSTMs.

Knowing that constituency parse tree can always be presented in binarized form, to compose a sentence the authors applied 2-ary Tree-LSTMs on the binarized constituency parse tree of that sentence. 
This combination was named by the authors as \textbf{Constituency Tree-LSTMs}~\cite{treeLSTM}.

Denoting sequence of words spanned by a sub-tree rooted at node \(j\) as \(\{x\}_j\). 
For both Dependency Tree-LSTMs and Constituency Tree-LSTMs when applying on the task of sentiment analysis, prediction at node \(j\) can be computed by a output-module as follow~\cite{treeLSTM}:

\begin{align}
  	\hat{p_{\theta}}(y \mid \{x\}_j ) &= softmax( W^{(s)} h_j + b^{(s)}) & \\
  	\hat{y_j} &= \underset{y}{\mathrm{argmax}} \; \hat{p_{\theta}}(y \mid \{x\}_j ) &
\end{align}

The authors use negative log-likelihood with \(L2\) regularization as loss function~\cite{treeLSTM}.

\paragraph{Training technique and hyper-parameters}
Glove vectors~\cite{glove} was used to initialize word-embedding. 
The words presentation were updated with learning rate 0.1 while other parameters in the network were update using AdaGrad~\cite{adagrad} with learning rate 0.05. 
Batch-size was set to 25. 
\(L2\) regularization was apply at each mini-batch with weight of \(10^{-4}\).
The authors also added a dropout layer~\cite{dropout} with dropout rate of \(0.5\) before each output-module.

\subsubsection{Results and Discussion}
\begin{table}[H]
\centering
\begin{tabular}{l c} 
 \hline
 \hline 
 Method & Accuracy \\ [0.5ex] 
 \hline
 \hline
 \\  
 RAE~\cite{socher2013recursive} & 82.4 \\ 
 MV-RNN~\cite{socher2013recursive} & 82.9 \\
 RNTN~\cite{socher2013recursive} & 85.4 \\
 DCNN~\cite{DCNN} & 86.8 \\
 Paragraph-Vec~\cite{ParagraphVec} & 87.8 \\
 CNN-non-static~\cite{KimCNN} & 87.2 \\
 CNN-multichannel~\cite{KimCNN} & 88.1 \\
 DRNN~\cite{IrsoyDRNN} & 86.6 \\ [0.5ex]
 \hline
 \\  
 LSTM~\cite{originLSTM} & 84.9 (0.6) \\ 
 Bidirectional LSTM~\cite{GravesLSTM} & 87.5 (0.5) \\
 2-layer LSTM~\cite{GravesLSTM} & 86.3 (0.6) \\
 2-layer Bidirectional LSTM~\cite{GravesLSTM} & 87.2 (1.0) \\ [0.5ex]
 \hline 
 \\  
 Dependency Tree-LSTM~\cite{treeLSTM} & 84.9 (0.6) \\ 
 Constituency Tree-LSTM~\cite{treeLSTM} &  \\ 
 \; with randomly initialized vectors & 82.0 (0.5) \\ 
 \; Glove vectors, fixed & 87.5 (0.8) \\
 \; Glove vectors, tuned & 88.0 (0.3) \\
 \hline
 \hline
\end{tabular}
\caption{Test set accuracies on Stanford Sentiment Treebank with binary setting.
In the first block, the results were reported in their original papers. 
The second block contains results produced by sequential models and tree-structured models for the third block. 
For each models in the second and third block, mean accuracy over 5 runs (standard deviation in parentheses) was reported}
\label{table:1}
\end{table}

The results presented in Table~\ref{table:1} support the hypothesis that tree-structured LSTMs are better than sequential LSTMs when apply on sequences which have nested grammar~\cite{treeVSseq}. 
We should notice that good word embedding give a great boost to the system~\cite{Luong_betterword}, fine-tune also beneficial.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.335]{figure/tree-vs-seq}
	\caption{Test accuracy on three experiments with increasing limited length of sentences in training data. 
The vertical axis is accuracy and the horizontal axis is limited length of sentences in testing data. 
Experiments were done with \cite{socher2013recursive}, TreeRNTN~\cite{socher2013recursive}, TreeLSTM~\cite{treeLSTM} and LSTM~\cite{originLSTM}. 
50 is the number of dimension of hidden stat \(h\). 
This data set was generated using an artificial recursively defined language~\cite{bowman-treevslstm}.} 
	\label{fig:tree-vs-seq}
\end{figure}

\textbf{Advantages of Tree-LSTMs} are presented in several studies~\cite{need-tree}~\cite{bowman-treevslstm}: \label{treelstm-advantage}
\begin{itemize}
\item In case the input sequence belong to recursively defined language, given only a small subset of the data with limited length sentences, tree structures model have better ability to generalize compare to sequential ones.
But when we increase the limited length sentences, the advantage of tree over sequential models decrease fast~\cite{bowman-treevslstm}. 
This can be demonstrated in Fig.~\ref{fig:tree-vs-seq}.
\item Tree can breakdown complicated sentences into simpler phrases, which make it easier for generalization~\cite{improve-presentation}~\cite{need-tree}.
\item Some features which are far apart when a sentence is presented as sequence become closer when it is presented as tree~\cite{need-tree}.
\end{itemize}


\textbf{Disadvantages of Tree-LSTMs} including:\label{treelstm-drawback}
\begin{itemize}
\item Sentences can be wrongly parsed, especially when comments are expressed in informal language.
The performance of the system depended on the parser being used.
\item When combining a sub-tree, that sub-tree have no information about its context. 
Compared to sequential structures, when combining a input (e.g. word, phrase), the network already have the left context information of that input~\cite{shift-reduce}.
\item A closer look at Eqs.~\eqref{eq1:6},\eqref{eq:12} reveal that, Tree-LSTMs have only a logistic regression at each node to capture features.
We think this is the reason why Kim's CNN~\ref{kim-cnn} can outperform TreeLSTMs.  
\item Tree-LSTMs can not be train using batch in parallelism. But this disadvantage can be overcome thank to Tree-LSTMs shift-reduce implementation~\cite{shift-reduce}.
\end{itemize}






\subsection{Convolutional Neural Networks for Sentence Classification}\label{kim-cnn}
In Table~\ref{table:1}, we can see that, the best model is not Constituency Tree-LSTM but CNN-multichannel~\cite{KimCNN}, which was originally presented in this paper. 
With only a one-layer \hyperref[sec:cnn]{Convolution Neural Network}, and general hyper-parameter tuning, the author was able to archive state-of-art~\footnote{in 2014} performance on several NLP tasks, which include sentiment analysis (\hyperref[sec:sst]{Stanford Sentiment Treebank}~\cite{socher2013recursive}). 
The paper also successfully adapted the idea of training CNN on multiple channels image in Computer Vision to training CNN on multi-channel words embedding for sentence~\footnote{Implementation of this paper can be found at: https://github.com/yoonkim/CNN\_sentence}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.33]{figure/sentencecnn}
	\caption{Structure of CNN with two word embedding channels of the same sentence}
	\label{fig:multi-cnn}
\end{figure}

\subsubsection{Method}
Let denote \(\bm{x_i \in \mathbb{R}^d}\) as a \(d\)-dimension vector presentation of word-\(i\)th in a sentence. 
Given a sentence \(\bm{s}\) of length \(n\), we can present a sub-sequence of words in the sentence which start at word-\(i\)th and end at word-\(j\)th as:
\begin{align}
	x_{i:j} &= x_i \oplus x_{i+1} \oplus ... \oplus x_{j} &\label{concat}
\end{align}
In Eq.\eqref{concat}, operator \(\bm{\oplus}\) do concatenation. Therefore, \(x_{i:j} \in \mathbb{R}^{d(j-i+1)}\). 

\paragraph{Convolution filter} \label{conv-filter} Now we are ready to define a filter. A filter with window size \(\bm{l}\) is a vector \(\bm{w \in \mathbb{R}^{ld}}\) which apply on vector presentations of word-\(i\)th to word-\((i+l-1)\)th through the following equation:
\begin{align}
	c_i &= f(w \cdot x_{i:i+l-1} + b) &\label{filter}
\end{align}

In Eq.\eqref{filter}, \(\bm{b \in \mathbb{R}}\) as bias term and \(\bm{f}\) is a activation function. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{figure/no_padding}
	\caption{No padding and unit strides policy when applying a filter on a matrix.}
	\label{fig:no_padding}
\end{figure}

By slicing the filter through the sentence we can get vector \(\bm{c}\) which can be view as a feature map of sentence \(s\). 
In this paper, the author use no padding and unit strides policy (Fig.~\ref{fig:no_padding}), which gives \(c \in \mathbb{R}^{n-l+1}\).

\paragraph{Max-over-time pooling} Given the feature map, the author then take the maximum value in it \(\bm{c^* = max\{c\}}\) (max-over-time pooling~\cite{nlp-scratch}), which is a sentence feature produced by the filter.
So, convoluting a filter with the sentence will produce one feature \(\bm{c^* \in \mathbb{R}}\).

\paragraph{Sentence presentation} Applying \(\bm{k}\) filters on the sentence, and we will have a feature vector \(\bm{p \in \mathbb{R}^k}\). 
In case of sentiment classification, the feature vector \(p\) can be fed to a classifier like the one in the last layer of the architect presented in Fig.~\ref{fig:multi-cnn}.

\paragraph{Multi-channel input} Given that the sentence is presented by a set of channels \(\bm{Z}\) and the network has \(\bm{k}\) filters. 
We will modify Eq.~\eqref{filter} as follow: 

\begin{align}
	\forall h \in Z, \; \; \hat{c}_{ih} &= f(w \cdot x_{i:i+l-1} + b)& \\
	c_i &= \sum_{h \in Z} \hat{c}_{ih}&
\end{align}

The rest of the system is unchanged compared to a single-channel one.
The whole process is illustrated in the first two layers of the architect in Fig.~\ref{fig:multi-cnn}.

The author did experiments with several variations:

\begin{itemize}
  	\item \textbf{CNN-rand:} One channel word embedding are randomly initialized and updated during the training process.\label{cnn-rand}
	\item \textbf{CNN-static:} One channel word embedding are initialized with word2vec~\cite{word2vec} and not updated during the training process even for unknown randomly initialized new word.\label{cnn-static}
	\item \textbf{CNN-non-static:} One channel word embedding are initialized with word2vec and updated during the training process.\label{cnn-non-static}
	\item \textbf{CNN-multichannel:} Two channels word embedding are initialized with word2vec. One of the channel is updated during the training process the other is kept static.\label{cnn-multichannel}
\end{itemize}
\paragraph{Training method and hyper-parameters} 
Rectifier~\cite{rectifier} were use as activation function.
The author used window size 3, 4 and 5, each of which have 100 filters, the max-over-time pooling layer will produce a 300-dimension sentence presentation vector. 
Dropout layer was use after the max-over-time pooling layer with dropout-rate of \(0.5\).  
Batch-size was set to 50. 
All weight vectors were normalized to have \(\norm{w}_2 = 3\) whenever \(\norm{w}_2 > 3\). 
Adadelta~\cite{adadelta} was used as optimizer of the networks.
When training on Stanford Sentiment Treebank, each labeled sub-tree's span was treated as an example.
At test phrase, each input is a sentence, and output is a sentiment prediction for that sentence.

\subsubsection{Results and Discussion}\label{kimcnn-drawback}
\begin{table}[H]
\centering
\begin{tabular}{l c} 
 \hline
 \hline 
 Method & Accuracy \\ [0.5ex] 
 \hline
 \hline
 \\  
 CNN-rand & 82.7 \\ 
 CNN-static & 86.8 \\ 
 CNN-non-static & 87.2 \\ 
 CNN-multichannel & 88.1 \\ 
 \hline
 \hline
\end{tabular}
\caption{Test set accuracies on Stanford Sentiment Treebank with binary setting. These models are presented in~\ref{cnn-multichannel}.
A comparison with models from different works has been presented in~\ref{table:1}}
\label{table:KimCNN}
\end{table}

Multiple channel with one static and one for fine-tuning help the network to better generalize.
There is a drawback in CNN-multichannel: although max-over-time pooling largely simplified the network (which is good for preventing over-fit), it only tell if a feature appear in a sentence or not, the information about position of the feature is ignored.\label{kim-drawback}
The next research in~\ref{cnn-rnn}  improve CNN-multichannel based on this drawback.
 

\subsection{Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts}\label{cnn-rnn}
As we have observed the drawback of max-over-time pooling layer in~\ref{kimcnn-drawback}, we will analyze how the authors of this paper tackled it.

\subsubsection{Method}
\begin{figure}[H]
	\centering
\includegraphics[scale=0.5]{figure/conv-word}
	\caption{A convoluting filter slicing through a sentence. 
	Padding was added.}
	\label{fig:conv-word}
\end{figure}

\paragraph{Preprocessing Sentence} We re-use the definition of convolution filter described in~\ref{conv-filter}.
Given a sentence \(s\), the authors first padding it with \((l-1)\) "dummy-words" on each end of the sentence, with \(l\) as the largest window size among all filters.
After that, the padded-sentence is padded on its right until reach the length of the longest padded sentence in the data set.
In other words, given that \(m\) is length of the longest sentence in SST, the input of a filter with window size \(l\) will always be a padded-sentence of length \((m + 2(l-1))\).
A filter of size \(k\) than slice through a padded-sentence with unit strides and product a feature map \(c \in \ \mathbb{R}^{m + 2l - k - 1}\).
This process in illustrated in Fig.~\ref{fig:conv-word}

\begin{figure}[H]
	\centering	\includegraphics[scale=0.4]{figure/2-max}
	\caption{Max pooling with window size 2 and strides 2 slicing through a feature map}
	\label{fig:2-max-pooling}
\end{figure}
  
\paragraph{Max pooling layer} A max pooling layer of window size 2, with strides 2, slice through each feature map, and reduce it size by a half.
We will denote this vector \(p \in \mathbb{R}^{\floor{\frac{m + 2l - k - 1}{2}}} \). 
This process in illustrated in Fig.~\ref{fig:2-max-pooling}   


\begin{figure}[H]
	\raggedleft	\includegraphics[scale=0.34]{figure/cnn-rnn}
	\caption{CNN-RNN}
	\label{fig:cnn-rnn}
\end{figure}

Note that for filters with different window sizes, the size of feature vectors \(p\) are also different. 
This will be problem if we want to compile feature vectors with different size.
To solve this problem, the authors chose only two types of window size: \(4\) and \(5\).
Which result in all feature vectors having the same size. 

Suppose we have \(n\) filters, authors then concatenated all the feature vectors into one matrix \(P \in \mathbb{R}^{n \times \floor{\frac{m + 2l - k - 1}{2}}}\), which each row is a feature vectors. 
After that, each column in \(P\) treated as an input \(i \in \mathbb{R}^{n}\) for a recurrent neural network, which can be RNN, LSTM or GRU.
The last output the recurrent neural network can be then feed to MLP for classification.
The whole process is demonstrated in Fig.~\ref{fig:cnn-rnn}.

\paragraph{Training method and hyper-parameters} 
Word presentation vectors was initialized with word2vec~\cite{word2vec}.
The author used window size 4 and 5, each of which have 200 filters. 
When training on Stanford Sentiment Treebank, each labeled sub-tree's span was treated as an example.
At test phrase, each input is a sentence, and output is a sentiment prediction for that sentence.
Other hyper-parameters were not specified in this paper, but can be found in the authors' implementation~\footnote{https://github.com/ultimate010/crnn}.

\subsubsection{Results and Discussion}
\begin{table}[H]
\centering
\begin{tabular}{l c} 
 \hline
 \hline 
 Method & Accuracy \\ [0.5ex] 
 \hline
 \hline
 \\  
 MAX-GRU & 89.95 \\ 
 MAX-LSTM & 89.56 \\ 
 AVG-GRU & 89.61 \\ 
 \hline
 \hline
\end{tabular}
\caption{Test set accuracies on Stanford Sentiment Treebank with binary setting. 
The models' names have the following format: \{type pooling layer\}-\{type of RNN\}.
A comparison with models from different works has been presented in~\ref{table:1}}
\label{table:cnn-rnn}
\end{table}

This paper prove that, the ability to combine feature based on their position is important for sentence-level sentiment analysis. 
This is one way to overcome the disadvantage in Yoon Kim's models~\ref{kim-drawback}.


\subsection{Unsupervised pre-training models}
A large number of studies have prove that unsupervised pretraining model can help it greatly generalize~\cite{why-unsupervised}~\cite{greedy-layer}~\cite{greedy-layer-bengio}~\cite{pretrain-1}.
In case of Sentiment Analysis and specifically the task on Stanford Sentiment Treebank, we can identify several challenges that can be overcome using unsupervised pre-train technique~\cite{why-unsupervised}:
\begin{itemize}
\item Given the training data set of SST, the network have no knowledge about film industry (e.g. movie genres, director, actor, art style). 
\item It also do not know human emotions or meaning of their expressions.
\item The network also have little knowledge about meaning of phrases, idioms, meaning of words based on its' context.
\item The network might have too many tuning parameters compare to the number of training examples in SST. 
Which can lead to over-fitting.
\end{itemize}
We will present several potential methods that are applicable for our models. 

\subsubsection{Natural Language Modeling}
The goal of language modeling is to model the probability distribution of a word conditioning on the words before it.
More concretely, given a sequence of words \((w_0, w_1,...,w_n)\), our model predict the distribution of \(P(w_{n+1}|w_0, w_1,...,w_n)\). 
For this task, we can use any model (e.g. recurent neural network, convolution neural network, MLP) that have ability to encode an arbitrary sequence of words/symbols (belong to our language).
The encoded context are then used to predict the next word \(w_{n+1}\).
With consideration, the whole system can be trained end-to-end.\footnote{Pytorch implementation: https://github.com/pytorch/examples/tree/master/word\_language\_model}

\textbf{The advantages} of this approach including:
\begin{itemize}
\item Can be trained on a large number of data sets, with the only condition those the data sets belong to the language.
\item If we pre-train our models on Amazon Reviews data set, the models can learn to capture sentiment features, as it need to guest the distribution of words in comments.
It could also learn more interesting features, such as: 
The reviewer cry when watching an good romantic movie, or the viewer praise the novel and then criticize the movie based on it, or co-occurrence of sentence structure and sentiment expressed.  
\end{itemize}

One clear disadvantage of this approach is that, we can not use it to unsupervised pre-train tree structured networks. 
As tree need full sentence with or full phrase, it can not process arbitrary words sequence. 




\subsubsection{Sentence Encoding}
In this approach, the whole sentence is encoded using some models that we wish to pre-train. 
The encoded sentence is then use to predict the words it contains.  
The prediction task can be defined in different ways.
We can use a small context window around the guesting word.
The words in context window and vector presentation of the sentence can be concatenated/averaged and feed to a classifier as in Fig.~\ref{fig:para-vec-1}.
Another way would be to just use the sentence presentation vector to guest which word it contain as in Fig.~\ref{fig:para-vec-2}


\begin{figure}[H]
	\centering	\includegraphics[scale=0.3]{figure/para-vec-1}
	\caption{Distributed Memory Model of Paragraph Vectors (PV-DM)~\cite{ParagraphVec}}
	\label{fig:para-vec-1}
\end{figure}

\begin{figure}[H]
	\centering	\includegraphics[scale=0.3]{figure/para-vec-2}
	\caption{Distributed Bag of Words version of Paragraph Vector (PV-DBOW)~\cite{ParagraphVec}}
	\label{fig:para-vec-2}
\end{figure}

\section{Improving continuous distributed word presentation}

\subsection{Duyu Tang's article on Twitter word embedding}
\subsubsection{Method}

\subsubsection{Results and Discussion}


\subsection{Misha Denil's article on hierarchical CNN}
\subsubsection{Method}

\subsubsection{Results and Discussion}


\subsection{Dimitrios Kotzias's article on MIL}
\subsubsection{Method}

\subsubsection{Results and Discussion}
